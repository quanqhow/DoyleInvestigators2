{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project2_doyle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edponce/DoyleInvestigators2/blob/main/rinehart_pipeline/project2_doyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU2UZc1BSNbL"
      },
      "source": [
        "# Import modules\n",
        "import requests\n",
        "import re, spacy\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pickle\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "%matplotlib inline\n",
        "import random\n",
        "from itertools import chain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3mBcLpLJbrv"
      },
      "source": [
        "import re  # For preprocessing\n",
        "import pandas as pd  # For data handling\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "\n",
        "import spacy  # For preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
        "\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oblo-6o87X2y"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnlbSYlR0vdQ"
      },
      "source": [
        "# Load word2vec models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQyMNg4_UzGm"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuBO0jzAYqIQ"
      },
      "source": [
        "doyle_w2v_sent_300 = pickle.load(open('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/models/doyle_w2v_model_dim300_window12_cb.sav', 'rb'))\n",
        "doyle_w2v_sent_50 = pickle.load(open('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/models/doyle_w2v_model_dim50_window12_cb.sav', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7DH0L22yBqE"
      },
      "source": [
        "# Load data from each author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgXbRbMg0xzO"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Rinehart_350_train.csv')\n",
        "df_chri_350_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Christie_350_train.csv').sample(n=int(len(df_train)/2.0), replace=False, random_state=1)\n",
        "df_doyl_350_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Doyle_350_train.csv').sample(n=int(len(df_train)/2.0), replace=False, random_state=1)\n",
        "\n",
        "df_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Rinehart_350_test.csv')\n",
        "df_chri_350_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Christie_350_test.csv')\n",
        "df_doyl_350_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Doyle_350_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_6aZqiNULFb"
      },
      "source": [
        "df_train1 = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Rinehart_1400_train.csv')\n",
        "df_chri_1400_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Christie_1400_train.csv').sample(n=int(len(df_train1)/2.0), replace=False, random_state=1)\n",
        "df_doyl_1400_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Doyle_1400_train.csv').sample(n=int(len(df_train1)/2.0), replace=False, random_state=1)\n",
        "\n",
        "df_test1 = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Rinehart_1400_test.csv')\n",
        "df_chri_1400_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Christie_1400_test.csv')\n",
        "df_doyl_1400_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Doyle_1400_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvpIGEq7ULDN"
      },
      "source": [
        "df_train2 = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Rinehart_3500_train.csv')\n",
        "df_chri_3500_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Christie_3500_train.csv').sample(n=int(len(df_train2)/2.0), replace=False, random_state=1)\n",
        "df_doyl_3500_train = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Doyle_3500_train.csv').sample(n=int(len(df_train2)/2.0), replace=False, random_state=1)\n",
        "\n",
        "df_test2 = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Rinehart_3500_test.csv')\n",
        "df_chri_3500_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Christie_3500_test.csv')\n",
        "df_doyl_3500_test = pd.read_csv('/content/drive/Shared drives/ECE-692 Rinehart Investigators /Project 2/data/data_documents/Doyle_3500_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWKW87yyV2W"
      },
      "source": [
        "# Training MLP models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJskL3n3UrS2"
      },
      "source": [
        "def w2v(df, model, dim):\n",
        "  data_ls = []\n",
        "  for i in range(len(df)):\n",
        "    t = df.loc[i].values.tolist()\n",
        "    data = []\n",
        "    for x in t:\n",
        "      if x in list(model.wv.vocab.keys()):\n",
        "        kn = model[x].tolist()\n",
        "        data.append(kn)\n",
        "      else:\n",
        "        unk = [0]*dim\n",
        "        data.append(unk)\n",
        "    data_ls.append(list(map(lambda x: sum(x)/len(x), zip(*data))))\n",
        "  return data_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNYha8KuSZiN"
      },
      "source": [
        "def training(df_train, df1, df2, model, dim):\n",
        "  df_train = df_train.append(df1, ignore_index=True)\n",
        "  df_train = df_train.append(df2, ignore_index=True)  \n",
        "  print(df_train.shape)\n",
        "\n",
        "  train_data = w2v(df_train, model, dim)\n",
        "  print(len(train_data))\n",
        "  df_tr = pd.DataFrame(train_data)\n",
        "  df_tr['author'] = df_train['Label'].values.tolist()\n",
        "\n",
        "  df_tr = df_tr.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  y_train = df_tr['author']\n",
        "  x_train = df_tr.drop(['author'], axis=1)\n",
        "\n",
        "  for i, x in enumerate(y_train.tolist()):\n",
        "    if (x == 'christie' or x == 'rinehart'): \n",
        "      y_train[i] = 'other'\n",
        "\n",
        "  print(np.unique(y_train))\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(300,300), max_iter=4000, alpha=0.0001,\n",
        "                    validation_fraction=0.1,\n",
        "                     solver='adam', verbose=10,  random_state=2,tol=0.000000001)\n",
        "  clf.fit(x_train, y_train)\n",
        "\n",
        "  return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdI_L_q_oIvo"
      },
      "source": [
        "# Path to save MLP models\n",
        "path = '/content/drive/My Drive/clf_doyle/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFVtb_Z1m219"
      },
      "source": [
        "## Train models with 50 dim embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttl-5vFFm_ng"
      },
      "source": [
        "clf_350_50 = training(df_train, df_chri_350_train, df_doyl_350_train, doyle_w2v_sent_50, 50)\n",
        "# filename = 'clf_350_50.sav'\n",
        "# pickle.dump(clf_350_50, open(path+filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E8aW9kenGTV"
      },
      "source": [
        "clf_1400_50 = training(df_train1, df_chri_1400_train,df_doyl_1400_train, doyle_w2v_sent_50, 50)\n",
        "# filename = 'clf_1400_50.sav'\n",
        "# pickle.dump(clf_1400_50, open(path+filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnYrxU1bnTre"
      },
      "source": [
        "clf_3500_50 = training(df_train2, df_chri_3500_train,df_doyl_3500_train, doyle_w2v_sent_50, 50)\n",
        "# filename = 'clf_3500_50.sav'\n",
        "# pickle.dump(clf_3500_50, open(path+filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "achWC3_nnPEK"
      },
      "source": [
        "## Train models with 300 dim embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZrE8eNpZR4C"
      },
      "source": [
        "clf_350_300 = training(df_train, df_chri_350_train, df_doyl_350_train, doyle_w2v_sent_300, 300)\n",
        "# filename = 'clf_350_300.sav'\n",
        "# pickle.dump(clf_350_300, open(path+filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbsKgYvHZSyj"
      },
      "source": [
        "clf_1400_300 = training(df_train1, df_chri_1400_train,df_doyl_1400_train, doyle_w2v_sent_300, 300)\n",
        "# filename = 'clf_1400_300.sav'\n",
        "# pickle.dump(clf_1400_300, open(path+filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEoEcUHCZTKg"
      },
      "source": [
        "clf_3500_300 = training(df_train2, df_chri_3500_train,df_doyl_3500_train, doyle_w2v_sent_300, 300)\n",
        "# filename = 'clf_3500_300.sav'\n",
        "# pickle.dump(clf_3500_300, open(path+filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84_dus30ypZy"
      },
      "source": [
        "# Testing MLP models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqrRnl7KfTAL"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1yyweL84j0k"
      },
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title,\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIE3CCiXTfsJ"
      },
      "source": [
        "def testing(df_test, df1, df2, model, clf, dim):\n",
        "  df_test = df_test.append(df1, ignore_index=True)\n",
        "  df_test = df_test.append(df2, ignore_index=True)  \n",
        "  test_data = w2v(df_test, model,dim)\n",
        "\n",
        "  df_te = pd.DataFrame(test_data)\n",
        "  df_te['author'] = df_test['Label'].values.tolist()\n",
        "\n",
        "  df_te = df_te.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  y_test = df_te['author']\n",
        "  x_test = df_te.drop(['author'], axis=1)\n",
        "\n",
        "  for i, x in enumerate(y_test.tolist()):\n",
        "    if (x == 'christie' or x == 'rinehart'): \n",
        "      y_test[i] = 'other'\n",
        "\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print('F1 Score: ', f1_score(y_test, y_pred, average='micro'))\n",
        "  plot_confusion_matrix(cm,\n",
        "                      normalize    = False,\n",
        "                      target_names = ['Others', 'Doyle'],\n",
        "                      title        = \"Confusion Matrix\")\n",
        "\n",
        "  return y_test, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YSkdH_cpu6K"
      },
      "source": [
        "# Path from where to load MLP models\n",
        "path = '/content/drive/My Drive/clf_doyle/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQjm_eyrp4zy"
      },
      "source": [
        "## Testing with 50 dim embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRgv2zrxU1OI"
      },
      "source": [
        "#filename = 'clf_350_50.sav'\n",
        "#clf_350_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_350_50, y_pred_350_50 = testing(df_test, df_chri_350_test, df_doyl_350_test, doyle_w2v_sent_50, clf_350_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvDdrG3qO0j"
      },
      "source": [
        "#filename = 'clf_1400_50.sav'\n",
        "#clf_1400_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_1400_50, y_pred_1400_50 = testing(df_test1, df_chri_1400_test, df_doyl_1400_test, doyle_w2v_sent_50, clf_1400_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu8kplV5qk8J"
      },
      "source": [
        "#filename = 'clf_3500_50.sav'\n",
        "#clf_3500_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_3500_50, y_pred_3500_50 = testing(df_test2, df_chri_3500_test, df_doyl_3500_test, doyle_w2v_sent_50, clf_3500_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgiwnE2Sq9UL"
      },
      "source": [
        "## Testing with 300 dim embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNRCcb1dU1Tc"
      },
      "source": [
        "#filename = 'clf_350_300.sav'\n",
        "#clf_350_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_350_300, y_pred_350_300 = testing(df_test, df_chri_350_test, df_doyl_350_test, doyle_w2v_sent_300, clf_350_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWgY538ZxE7I"
      },
      "source": [
        "#filename = 'clf_1400_300.sav'\n",
        "#clf_1400_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_1400_300, y_pred_1400_300 = testing(df_test1, df_chri_1400_test, df_doyl_1400_test, doyle_w2v_sent_300, clf_1400_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O9qDy4XxRKz"
      },
      "source": [
        "#filename = 'clf_3500_300.sav'\n",
        "#clf_3500_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_3500_300, y_pred_3500_300 = testing(df_test2, df_chri_3500_test, df_doyl_3500_test, doyle_w2v_sent_300, clf_3500_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbl4o1Cwrmis"
      },
      "source": [
        "# Data Perturbation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsGMna2P9xcl"
      },
      "source": [
        "def pert_testing(df_test, model, clf, dim):\n",
        "  test_data = w2v(df_test, model,dim)\n",
        "\n",
        "  df_te = pd.DataFrame(test_data)\n",
        "  df_te['author'] = df_test['Label'].values.tolist()\n",
        "\n",
        "  df_te = df_te.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  y_test = df_te['author']\n",
        "  x_test = df_te.drop(['author'], axis=1)\n",
        "\n",
        "  for i, x in enumerate(y_test.tolist()):\n",
        "    if (x == 'christie' or x == 'rinehart'): \n",
        "      y_test[i] = 'other'\n",
        "\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print('F1 Score: ', f1_score(y_test, y_pred, average='micro'))\n",
        "  plot_confusion_matrix(cm,\n",
        "                      normalize    = False,\n",
        "                      target_names = ['Others', 'Doyle'],\n",
        "                      title        = \"Confusion Matrix\")\n",
        "\n",
        "  return y_test, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXucQnAs-so"
      },
      "source": [
        "import json\n",
        "def load_corpus(fn: str) -> list:\n",
        "    with open(fn) as fd:\n",
        "        return json.load(fd)\n",
        "\n",
        "\n",
        "def json_df (test_file):\n",
        "  # Load documents from JSON file\n",
        "  docs = load_corpus(test_file)\n",
        "  num_docs = len(docs)\n",
        "  print('Loaded', num_docs, 'documents for testing')\n",
        "\n",
        "  # Process documents\n",
        "  # Each document is represented as a dictionary with a 'label' and 'text' field\n",
        "  text = []\n",
        "  label = []\n",
        "  for i, doc in enumerate(docs):\n",
        "    text.append(re.split('#| |\\--|\\n', doc['text'])) \n",
        "    #text.append(re.split('#|\\--|\\n', doc['text']))\n",
        "    label.append(doc['label'])\n",
        "\n",
        "  #df = pd.DataFrame(text, columns = ['word_'+str(i) for i in range(len(text[0]))])\n",
        "  df = pd.DataFrame(text)\n",
        "  df['Label'] = label\n",
        "  #print(df)\n",
        "  return(df)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAaGQ4K7zSUO"
      },
      "source": [
        "## Load JSON files to df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRC51Br_G4XY"
      },
      "source": [
        "#Add the perturbed data paths here\n",
        "#Read perturbed data\n",
        "paths_perturbed_data = [\n",
        "  '/content/drive/My Drive/perturbed_data_doyle/perturbed_numericals_doyle_350.json', \n",
        "  '/content/drive/My Drive/perturbed_data_doyle/perturbed_numericals_doyle_1400.json',\n",
        "  '/content/drive/My Drive/perturbed_data_doyle/perturbed_numericals_doyle_3500.json',\n",
        "  '/content/drive/My Drive/perturbed_data_doyle/perturbed_synonym_doyle_350.json',\n",
        "  '/content/drive/My Drive/perturbed_data_doyle/perturbed_synonym_doyle_1400.json',\n",
        "  '/content/drive/My Drive/perturbed_data_doyle/perturbed_synonym_doyle_3500.json',\n",
        "  #'/content/drive/My Drive/perturbed_data_doyle/perturbed_synonym_rinehart_350.json',\n",
        "  #'/content/drive/My Drive/perturbed_data_doyle/perturbed_synonym_rinehart_1400.json',\n",
        "  #'/content/drive/My Drive/perturbed_data_doyle/perturbed_synonym_rinehart_3500.json',\n",
        "]\n",
        "               \n",
        "df_pert_350_num = json_df(paths_perturbed_data[0])\n",
        "df_pert_1400_num = json_df(paths_perturbed_data[1]) \n",
        "df_pert_3500_num = json_df(paths_perturbed_data[2])\n",
        "\n",
        "df_pert_350_syn = json_df(paths_perturbed_data[3])\n",
        "df_pert_1400_syn = json_df(paths_perturbed_data[4]) \n",
        "df_pert_3500_syn = json_df(paths_perturbed_data[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_eDBlcOvKZx"
      },
      "source": [
        "## Testing with 50 dim embedding - Synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3yjpAN2U1Rf"
      },
      "source": [
        "#filename = 'clf_350_50.sav'\n",
        "#clf_350_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_350_50_syn, y_pred_350_50_syn = pert_testing(df_pert_350_syn, doyle_w2v_sent_50, clf_350_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KacXZvLUyIUV"
      },
      "source": [
        "#filename = 'clf_1400_50.sav'\n",
        "#clf_1400_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_1400_50_syn, y_pred_1400_50_syn = pert_testing(df_pert_1400_syn, doyle_w2v_sent_50, clf_1400_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NGT1cQRyrEK"
      },
      "source": [
        "#filename = 'clf_3500_50.sav'\n",
        "#clf_3500_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_3500_50_syn, y_pred_3500_50_syn = pert_testing(df_pert_3500_syn, doyle_w2v_sent_50, clf_3500_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnETT8-TvZ8i"
      },
      "source": [
        "## Testing with 50 dim embedding - Numericals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5oTyUVFy84O"
      },
      "source": [
        "#filename = 'clf_350_50.sav'\n",
        "#clf_350_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_350_50_num, y_pred_350_50_num = pert_testing(df_pert_350_num, doyle_w2v_sent_50, clf_350_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMAJaD_Jy84n"
      },
      "source": [
        "#filename = 'clf_1400_50.sav'\n",
        "#clf_1400_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_1400_50_num, y_pred_1400_50_num = pert_testing(df_pert_1400_num, doyle_w2v_sent_50, clf_1400_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HytEHhMWy843"
      },
      "source": [
        "#filename = 'clf_3500_50.sav'\n",
        "#clf_3500_50 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_3500_50_num, y_pred_3500_50_num = pert_testing(df_pert_3500_num, doyle_w2v_sent_50, clf_3500_50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8-HeSEJvXeX"
      },
      "source": [
        "## Testing with 300 dim embedding - Synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBk_EC7yzRez"
      },
      "source": [
        "#filename = 'clf_350_300.sav'\n",
        "#clf_350_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_350_300_syn, y_pred_350_300_syn = pert_testing(df_pert_350_syn, doyle_w2v_sent_300, clf_350_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfEF1Si7zRfG"
      },
      "source": [
        "#filename = 'clf_1400_300.sav'\n",
        "#clf_1400_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_1400_300_syn, y_pred_1400_300_syn = pert_testing(df_pert_1400_syn, doyle_w2v_sent_300, clf_1400_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li7aTjTGzRfR"
      },
      "source": [
        "#filename = 'clf_3500_300.sav'\n",
        "#clf_3500_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_3500_300_syn, y_pred_3500_300_syn = pert_testing(df_pert_3500_syn, doyle_w2v_sent_300, clf_3500_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eUCxnDGveLX"
      },
      "source": [
        "## Testing with 300 dim embedding - Numericals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jc42g6kzWCt"
      },
      "source": [
        "#filename = 'clf_350_300.sav'\n",
        "#clf_350_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_350_300_num, y_pred_350_300_num = pert_testing(df_pert_350_num, doyle_w2v_sent_300, clf_350_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zadUss4AzWC_"
      },
      "source": [
        "#filename = 'clf_1400_300.sav'\n",
        "#clf_1400_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_1400_300_num, y_pred_1400_300_num = pert_testing(df_pert_1400_num, doyle_w2v_sent_300, clf_1400_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nngGxYmFzWDK"
      },
      "source": [
        "#filename = 'clf_3500_300.sav'\n",
        "#clf_3500_300 = pickle.load(open(path+filename, 'rb'))\n",
        "y_test_3500_300_num, y_pred_3500_300_num = pert_testing(df_pert_3500_num, doyle_w2v_sent_300, clf_3500_300, 300)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}