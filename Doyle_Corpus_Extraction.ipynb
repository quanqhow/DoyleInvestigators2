{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edponce/DoyleInvestigators2/blob/main/Doyle_Corpus_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2wdHfSTKj4"
      },
      "source": [
        "#$\\color{brown}{\\rm Team~Members}$\n",
        "## Jerry Duncan\n",
        "## Fabian Fallas\n",
        "## Chris Gropp\n",
        "## Maofeng Tang\n",
        "## Quan Zhou\n",
        "## Eduardo Ponce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv7gPdfwPSxq"
      },
      "source": [
        "#$\\color{brown}{\\rm Imports}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D64o3VubPHvS"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import copy\n",
        "import gensim\n",
        "import urllib.request\n",
        "import urllib.parse"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-XOfdD4g5lO"
      },
      "source": [
        "#$\\color{brown}{\\rm Corpus~Selection}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbl0IcL4PQGL"
      },
      "source": [
        "CORPUS_URL = {\n",
        "    'The Valley of Fear': \"http://www.gutenberg.org/files/3776/3776.txt\",\n",
        "    'A Study in Scarlet': \"http://www.gutenberg.org/files/244/244.txt\",\n",
        "    'The Sign of the Four': \"http://www.gutenberg.org/files/2097/2097.txt\",\n",
        "    'The Hound of the Baskervilles': \"http://www.gutenberg.org/files/2852/2852.txt\",\n",
        "    # NOTE: These stories are part of a compilation, so include (URL, story # in compilation)\n",
        "    'The Boscombe Valley Mystery': ('https://www.gutenberg.org/files/1661/1661.txt', 4),\n",
        "    'The Five Orange Pips': ('https://www.gutenberg.org/files/1661/1661.txt', 5),\n",
        "    'The Adventure of the Speckled Band': ('https://www.gutenberg.org/files/1661/1661.txt', 8),\n",
        "    'The Adventure of the Cardboard Box': ('https://www.gutenberg.org/files/834/834-0.txt', 2),\n",
        "    'The Musgave Ritual': ('https://www.gutenberg.org/files/834/834-0.txt', 6),\n",
        "    'The Reigate Squires': ('https://www.gutenberg.org/files/834/834-0.txt', 7),\n",
        "    'The Adventure of the Dancing Men': ('https://www.gutenberg.org/files/221/221.txt', 3),\n",
        "    'The Adventure of the Second Stain': ('https://www.gutenberg.org/files/221/221.txt', 13),\n",
        "}"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbEAIVRj7lbu"
      },
      "source": [
        "#$\\color{brown}{\\rm Load~Corpus}$\n",
        "Read a corpus from web page or file to start processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AIatw6d7gUZ"
      },
      "source": [
        "def get_corpus_from_url(url):\n",
        "    with urllib.request.urlopen(url) as fd:\n",
        "        text = fd.read()\n",
        "        try:\n",
        "            return text.decode('utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            return text.decode('iso-8859-1')\n",
        "\n",
        "\n",
        "def get_corpus_from_file(file):\n",
        "    with open(file) as fd:\n",
        "        return fd.read()\n",
        "\n",
        "\n",
        "def get_corpus(key):\n",
        "    def validate_url(url):\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        return all([parsed_url.scheme, parsed_url.netloc, parsed_url.path])\n",
        "\n",
        "    # Check if a filename was provided\n",
        "    if os.path.isfile(key):\n",
        "        return get_corpus_from_file(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            fn = CORPUS_URL[key]\n",
        "            if isinstance(fn, (list, tuple)):\n",
        "                fn = fn[0]\n",
        "            for file in (fn, os.path.basename(fn)):\n",
        "                if os.path.isfile(fn):\n",
        "                    return get_corpus_from_file(file)\n",
        "\n",
        "    # Check if a URL was provided\n",
        "    if validate_url(key):\n",
        "        return get_corpus_from_url(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            url = CORPUS_URL[key]\n",
        "            if isinstance(url, (list, tuple)):\n",
        "                url = url[0]\n",
        "            if validate_url(url):\n",
        "                return get_corpus_from_url(url)\n",
        "\n",
        "    raise Exception(f\"corpus '{key}' not found\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q8QE-jl8q-0"
      },
      "source": [
        "#$\\color{brown}{\\rm Headings~Detection~(Regex)}$\n",
        "Functions to get spans of headings:\n",
        "* Gutenberg tags\n",
        "* Named headings - parts, chapters, adventures\n",
        "* Numbered headings\n",
        "* Epilogue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFVpoNyQTRp0"
      },
      "source": [
        "def get_newline_index(text):\n",
        "    \"\"\"Find the index of the first newline in the text.\n",
        "    This is used to skip/correct one newline at beginning of headings.\n",
        "    \"\"\"\n",
        "    match = re.match(r'[ \\t\\r]*\\n', text)\n",
        "    return match.end() if match else 0\n",
        "\n",
        "\n",
        "def get_gutenberg_start_heading(text, span=None):\n",
        "    \"\"\"Find Gutenberg's start tag (and producer, if available).\n",
        "\n",
        "    Notes:\n",
        "        * re.match() searches at the beginning of strings, but there are\n",
        "          certain character combinations that are not considered strings,\n",
        "          and thus need to use re.search(), even if it is at the beginning\n",
        "          of line. An example are the asterisks in the Gutenberg START\n",
        "          tag.\n",
        "    \"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    match = re.search(\n",
        "        r'(^\\s*|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r'start[^\\r\\n]+'  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'(\\s*\\nproduced by.+)?'  # producer line\n",
        "        r'(\\s*\\n){2,}',  # post-whitespace\n",
        "        text[span[0]:span[1]],\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        span = match.span()\n",
        "        offs = get_newline_index(text[span[0]:span[1]])\n",
        "        return span[0] + offs, span[1]\n",
        "\n",
        "\n",
        "def get_gutenberg_end_heading(text, span=None):\n",
        "    \"\"\"Find Gutenberg's end tag (and transcriber's notes, if available).\n",
        "\n",
        "    Notes:\n",
        "        * Duplicate/similar Gutenberg end tags.\n",
        "        * Use a newline before transcriber note to prevent matching similar\n",
        "          (but indented) notes at beginning of text.\n",
        "        * Use DOTALL flag to match transcriber's notes across multiple lines.\n",
        "          But be wary that using DOTALL prevents the use of '.+' for other\n",
        "          cases, so use '[^\\r\\n]' instead.\n",
        "    \"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    match = re.search(\n",
        "        r'('\n",
        "        r'(\\s*\\n){2,}'  # pre-whitespace, no indentation\n",
        "        r'(original transcriber.+\\s*\\n)?'  # transcriber notes\n",
        "        r'end[^\\r\\n]+'  # duplicate/similar tag text\n",
        "        r')?'\n",
        "        r'(\\s*\\n){2,}'  # pre-whitespace, no indentation\n",
        "        r'('\n",
        "        r'end of the project gutenberg.+'  # tag text\n",
        "        r'(\\s*\\n){2,}'  # post-whitespace\n",
        "        r')?'\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r\"end[^\\r\\n]+\"  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'(\\s*\\n){2,}',  # post-whitespace\n",
        "        text[span[0]:span[1]],\n",
        "        flags=re.DOTALL,\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        span = match.span()\n",
        "        offs = get_newline_index(text[span[0]:span[1]])\n",
        "        return span[0] + offs, span[1]\n",
        "\n",
        "\n",
        "def get_named_headings(text, name, span=None):\n",
        "    \"\"\"Find named headings with title.\"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    spans = [\n",
        "        (match.start() + span[0], match.end() + span[0])\n",
        "        for match in re.finditer(\n",
        "            r'(^(\\s*)|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "            r'('\n",
        "            fr'{name}[ \\t]+(\\d+|[ivxlcd]+)'  # label with Arabic/Roman number\n",
        "            r'(-+|\\.)?'  # label-title delimiter\n",
        "            r'((\\s*\\n){2})?'  # whitespace for titles two line apart\n",
        "            r'.*(\\r?\\n.*)?'  # title (muti-line support)\n",
        "            r'|'  # cases: name # \\s* label, # name/label\n",
        "            r'(\\d+|[ivxlcd]+)'  # label with Arabic or Roman numbering\n",
        "            r'(-+|\\.)?'  # label-title delimiter\n",
        "            fr'.*{name}.*'  # label with name\n",
        "            r')'\n",
        "            r'(\\s*\\n){2,}',  # post-whitespace\n",
        "            text[span[0]:span[1]],\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    _spans = []\n",
        "    for _span in spans:\n",
        "        offs = get_newline_index(text[_span[0]:_span[1]])\n",
        "        _spans.append((_span[0] + offs, _span[1]))\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def get_numbered_headings(text, span=None):\n",
        "    \"\"\"Find numbered headings with no title.\"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    spans = [\n",
        "        (match.start() + span[0], match.end() + span[0])\n",
        "        for match in re.finditer(\n",
        "            r'(^\\s*|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "            fr'(\\d+|[ivxlcd]+)'  # label with Arabic or Roman numbering\n",
        "            r'(-+|\\.)'  # label-title delimiter\n",
        "            r'([ \\t]+\\w+.*)?'  # label\n",
        "            r'(\\s*\\n){2,}',  # post-whitespace\n",
        "            text[span[0]:span[1]]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    _spans = []\n",
        "    for _span in spans:\n",
        "        offs = get_newline_index(text[_span[0]:_span[1]])\n",
        "        _spans.append((_span[0] + offs, _span[1]))\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def get_epilogue_heading(text, span=None):\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    match = re.search(\n",
        "        r'(^\\s*|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "        r'epilogue'  # tag text\n",
        "        r'(\\s*\\n){2,}',  # post-whitespace\n",
        "        text[span[0]:span[1]]\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        span = match.span()\n",
        "        offs = get_newline_index(text[span[0]:span[1]])\n",
        "        return span[0] + offs, span[1]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoh4upkIhTf_"
      },
      "source": [
        "#$\\color{brown}{\\rm Regions~of~Interest~(ROI)}$\n",
        "Functions to get spans of text between headings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH5HWEjvKShk"
      },
      "source": [
        "def get_headings_map(\n",
        "    text,\n",
        "    headings=['part', 'chapter', 'adventure', 'epilogue', 'numbered'],\n",
        "):\n",
        "    \"\"\"Create a list of all heading spans, guarantees at least one set\n",
        "    of bounding spans.\n",
        "\n",
        "    Args:\n",
        "        headings (str, List[str]): Heading names to search for.\n",
        "    \"\"\"\n",
        "    if not isinstance(headings, (list, tuple, set)):\n",
        "        _headings = [headings]\n",
        "    else:\n",
        "        _headings = copy.deepcopy(headings)\n",
        "\n",
        "    headings_map = {}\n",
        "    _headings_map = {}\n",
        "\n",
        "    # Always available heading, all text\n",
        "    text_heading = '_text_'\n",
        "\n",
        "    # Ensure there is always a begin \"span\"\n",
        "    start_span = get_gutenberg_start_heading(text)\n",
        "    if not start_span:\n",
        "        start_span = 0, 0\n",
        "\n",
        "    # Ensure there is always an end \"span\"\n",
        "    end_span = get_gutenberg_end_heading(text)\n",
        "    if not end_span:\n",
        "        end_span = len(text), len(text)\n",
        "    headings_map[text_heading] = [start_span, end_span]\n",
        "    if text_heading in _headings:\n",
        "        _headings.remove(text_heading)\n",
        "\n",
        "    # Optional\n",
        "    span = get_epilogue_heading(text)\n",
        "    if span:\n",
        "        heading = 'epilogue'\n",
        "        _headings_map[heading] = [span, headings_map[text_heading][1]]\n",
        "        if heading in _headings:\n",
        "            headings_map[heading] = _headings_map[heading]\n",
        "            _headings.remove(heading)\n",
        "\n",
        "    # Optional\n",
        "    spans = get_numbered_headings(text)\n",
        "    if spans:\n",
        "        heading = 'numbered'\n",
        "        _headings_map[heading] = [*spans, headings_map[text_heading][1]]\n",
        "        if heading in _headings:\n",
        "            headings_map[heading] = _headings_map[heading]\n",
        "            _headings.remove(heading)\n",
        "\n",
        "    # Optional\n",
        "    for heading in _headings:\n",
        "        spans = get_named_headings(text, heading)\n",
        "        if spans:\n",
        "            headings_map[heading] = spans\n",
        "            if 'epilogue' in _headings_map:\n",
        "                headings_map[heading].append(_headings_map['epilogue'][0])\n",
        "            else:\n",
        "                headings_map[heading].append(headings_map[text_heading][1])\n",
        "    return headings_map\n",
        "\n",
        "\n",
        "def select_rois_spans(spans, n=None):\n",
        "    if n is None:\n",
        "        _spans = [\n",
        "            (spans[i][1], spans[i + 1][0])\n",
        "            for i in range(len(spans) - 1)\n",
        "        ]\n",
        "    else:\n",
        "        _spans = [\n",
        "            (spans[i - 1][1], spans[i][0])\n",
        "            for i in ([n] if isinstance(n, int) else n)\n",
        "            if i >= 1 and i < (len(spans))\n",
        "        ]\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def remove_embedded_spans(spans):\n",
        "    non_embedded_spans = copy.deepcopy(spans)\n",
        "    for i in range(len(spans)):\n",
        "        span = spans[i]\n",
        "        for j in range(i + 1, len(spans)):\n",
        "            _span = spans[j]\n",
        "            if span[0] >= _span[0] and span[1] <= _span[1]:\n",
        "                non_embedded_spans.remove(span)\n",
        "                break\n",
        "            elif span[1] > _span[1]:\n",
        "                break\n",
        "    non_embedded_spans.sort()\n",
        "    return non_embedded_spans\n",
        "\n",
        "\n",
        "def get_nonoverlapped_spans(spans, *, join=True):\n",
        "    \"\"\"Remove fully embedded spans and join overlapped spans.\"\"\"\n",
        "    non_embedded_spans = remove_embedded_spans(spans)\n",
        "    non_embedded_spans = remove_embedded_spans(non_embedded_spans[::-1])\n",
        "    if not join:\n",
        "        return non_embedded_spans\n",
        "\n",
        "    joined_spans = []\n",
        "    for span in non_embedded_spans:\n",
        "        for _span in non_embedded_spans:\n",
        "            if span != _span:\n",
        "                joined_span = None\n",
        "                if span[0] >= _span[0] and span[0] <= _span[1]:\n",
        "                    joined_span = (_span[0], span[1])\n",
        "                elif span[1] >= _span[0] and span[1] <= _span[1]:\n",
        "                    joined_span = (span[0], _span[1])\n",
        "                if joined_span:\n",
        "                    if joined_span not in joined_spans:\n",
        "                        joined_spans.append(joined_span)\n",
        "                    break\n",
        "        else:\n",
        "            joined_spans.append(span)\n",
        "\n",
        "    nonoverlap_spans = sorted(joined_spans)\n",
        "\n",
        "    # Recurse until condition is satisfied\n",
        "    if nonoverlap_spans == spans:\n",
        "        return nonoverlap_spans\n",
        "    return get_nonoverlapped_spans(nonoverlap_spans)\n",
        "\n",
        "\n",
        "def contains_span(spans, span):\n",
        "    \"\"\"Validate if a span is contained in a collection of spans.\"\"\"\n",
        "    for _span in spans:\n",
        "        if span[0] >= _span[0] and span[1] <= _span[1]:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_rois(text, name=None, *, n=None, headings_map=None):\n",
        "    \"\"\"Get span bounding a ROI.\n",
        "\n",
        "    Args:\n",
        "        name (str): ROI\n",
        "\n",
        "        n (int, Iterable[int]): Number of ROI, [1,N]\n",
        "    \"\"\"\n",
        "    if not headings_map:\n",
        "        headings_map = get_headings_map(text)\n",
        "\n",
        "    # Always available heading, all text\n",
        "    text_heading = '_text_'\n",
        "\n",
        "    rois = []\n",
        "    if not name:\n",
        "        rois = [(\n",
        "            headings_map[text_heading][0][1],\n",
        "            headings_map[text_heading][1][0],\n",
        "        )]\n",
        "    elif name in headings_map:\n",
        "        rois = select_rois_spans(headings_map[name], n)\n",
        "\n",
        "    # If necessary, skip last inner heading\n",
        "    _rois = []\n",
        "    for roi in rois:\n",
        "        value = roi[1]\n",
        "        for spans in headings_map.values():\n",
        "            for span in spans:\n",
        "                if roi[1] > span[0] and roi[1] <= span[1]:\n",
        "                    value = span[0]\n",
        "        _rois.append((roi[0], value))\n",
        "    return _rois\n",
        "\n",
        "\n",
        "\n",
        "def get_roi(text, name, span=None, *, n=None):\n",
        "    if not span:\n",
        "        spans = get_rois(text, name, n=n)\n",
        "    else:\n",
        "        spans = [\n",
        "            (_span[0] + span[0], _span[1] + span[0])\n",
        "            for _span in get_rois(text[span[0]:span[1]], name, n=n)\n",
        "        ]\n",
        "    return spans\n",
        "\n",
        "\n",
        "def get_end_of_roi(text, regex, span=None):\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "    return [\n",
        "        (match.start() + span[0], match.end() + span[0])\n",
        "        for match in re.finditer(\n",
        "            regex,\n",
        "            text[span[0]:span[1]],\n",
        "        )\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_text_from_span(text, span=None):\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "    elif isinstance(span[0], int):\n",
        "        span = [span]\n",
        "\n",
        "    roi = ''\n",
        "    for _span in span:\n",
        "        roi += text[_span[0]:_span[1]]\n",
        "    return roi\n",
        "\n",
        "\n",
        "def get_text(text, span=None, *, n=None):\n",
        "    return get_rois(text)\n",
        "\n",
        "\n",
        "def get_parts(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'part', span, n=n)\n",
        "\n",
        "\n",
        "def get_chapters(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'chapter', span, n=n)\n",
        "\n",
        "\n",
        "def get_adventures(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'adventure', span, n=n)\n",
        "\n",
        "\n",
        "def get_numbered_sections(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'numbered', span, n=n)\n",
        "\n",
        "\n",
        "def get_epilogue(text, span=None):\n",
        "    return get_roi(text, 'epilogue', span)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usUdUT1ttRQa"
      },
      "source": [
        "#$\\color{brown}{\\rm Tokenization~(Regex)}$\n",
        "Regexes to get text decomposition:\n",
        "* Paragraphs\n",
        "* Sentences\n",
        "* Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJlJ_6-2tNR7"
      },
      "source": [
        "def tokenize(text, span=None, regex=r'\\w', *, use_remaining=False):\n",
        "    def _get_tokens(text):\n",
        "        return [\n",
        "            match.span()\n",
        "            for match in re.finditer(regex, text)\n",
        "        ]\n",
        "\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    # Get tokens from text\n",
        "    # Add base offset to tokens' spans\n",
        "    tokens = [\n",
        "        (tok_span[0] + span[0], tok_span[1] + span[0])\n",
        "        for tok_span in _get_tokens(text[span[0]:span[1]])\n",
        "    ]\n",
        "\n",
        "    if use_remaining:\n",
        "        if tokens:\n",
        "            # Extend last token to end of text\n",
        "            tokens[-1] = tokens[-1][0], span[1]\n",
        "        else:\n",
        "            # Consider all text as the token\n",
        "            tokens = [span]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def select_spans(spans, n=None):\n",
        "    if n is None:\n",
        "        _spans = spans\n",
        "    else:\n",
        "        _spans = [\n",
        "            spans[i - 1]\n",
        "            for i in ([n] if isinstance(n, int) else n)\n",
        "            if i >= 1 and i <= (len(spans))\n",
        "        ]\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def get_paragraphs(text, span=None, *, n=None):\n",
        "    spans = tokenize(\n",
        "        text,\n",
        "        span,\n",
        "        r'('\n",
        "        r'([^\\r\\n]+\\r?\\n)+'  # (regular text with newline)+\n",
        "        r'('\n",
        "        r'(\\r?\\n)+'  # (newline)+\n",
        "        r'[^a-zA-Z]'  # non-alpha character: quote, number, etc.\n",
        "        r')?'  # handles case of multiple newlines but still same paragraph\n",
        "        r')+',  # (full regex)+\n",
        "        use_remaining=True,\n",
        "    )\n",
        "    return select_spans(spans, n)\n",
        "\n",
        "\n",
        "def get_sentences(text, span=None, *, n=None):\n",
        "    spans = tokenize(\n",
        "        text,\n",
        "        span,\n",
        "        r'('\n",
        "        r'([^\\.\\r\\n;M!]+(\\r?\\n)?)+'\n",
        "        r'(.\")?'\n",
        "        r'(M[rR][sS]?\\.\\s)?'\n",
        "        r'M?'\n",
        "        r')+'\n",
        "        r'|'\n",
        "        r'M[rR][sS]?'\n",
        "        r'\\.\\s'\n",
        "        r'([^\\.;M!]+(.\")?(M[rR][sS]?\\.\\s)?(M)?)+',\n",
        "    )\n",
        "    return select_spans(spans, n)\n",
        "\n",
        "\n",
        "def get_tokens(text, span=None, *, n=None):\n",
        "    spans = tokenize(\n",
        "        text,\n",
        "        span,\n",
        "        r'\\w+'  # compound alphanumeric words\n",
        "        r'('\n",
        "        r\"'\\w+\"  # contractions\n",
        "        r'|(-\\w+)+'  # tokens with inlined dashes\n",
        "        r')'\n",
        "        r'|\\w+'  # single alphanumeric words\n",
        "        r'|\\$?-?\\d+(,\\d+)*(\\.\\d+)?',  # numbers, decimals, monetary\n",
        "    )\n",
        "    return select_spans(spans, n)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J26kopyFSL6C"
      },
      "source": [
        "#$\\color{brown}{\\rm Drivers}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXKW-c1nfhL"
      },
      "source": [
        "# Get merged text from stories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXKcNMAQnw-B"
      },
      "source": [
        "def clean_text_ws(text):\n",
        "    # Remove newlines and extra whitespaces\n",
        "    return re.sub(r'\\s+', ' ', text)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkD_jNoUqrkX"
      },
      "source": [
        "texts = {}"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PpNHV7AkGHG"
      },
      "source": [
        "for story in ('The Valley of Fear', 'A Study in Scarlet'):\n",
        "    corpus = get_corpus(story)\n",
        "    corpus_l = corpus.lower()\n",
        "    text = ''\n",
        "    for part_span in get_parts(corpus_l):\n",
        "        for chp_span in get_chapters(corpus_l, part_span):\n",
        "            text += get_text_from_span(corpus, chp_span)\n",
        "            text += '\\n'\n",
        "    epilogue_span = get_epilogue(corpus_l)\n",
        "    if epilogue_span:\n",
        "        text += get_text_from_span(corpus, epilogue_span)\n",
        "    texts[story] = text"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAawge9Kt6_O"
      },
      "source": [
        "for story in ('The Sign of the Four', 'The Hound of the Baskervilles'):\n",
        "    corpus = get_corpus(story)\n",
        "    corpus_l = corpus.lower()\n",
        "    text = ''\n",
        "    for chp_span in get_chapters(corpus_l):\n",
        "        text += get_text_from_span(corpus, chp_span)\n",
        "        text += '\\n'\n",
        "    texts[story] = text"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwej2kOsvH3Z"
      },
      "source": [
        "for story in ('The Boscombe Valley Mystery', 'The Five Orange Pips', 'The Adventure of the Speckled Band'):\n",
        "    corpus = get_corpus(story)\n",
        "    texts[story] = get_text_from_span(corpus, get_adventures(corpus.lower(), n=CORPUS_URL[story][1]))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU8Qs6eF-xV2"
      },
      "source": [
        "for story in ('The Adventure of the Cardboard Box', 'The Musgave Ritual', 'The Reigate Squires'):\n",
        "    corpus = get_corpus(story)\n",
        "    texts[story] = get_text_from_span(corpus, get_numbered_sections(corpus.lower(), n=CORPUS_URL[story][1]))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlKW8qRTL_LO"
      },
      "source": [
        "for story in ('The Adventure of the Dancing Men', 'The Adventure of the Second Stain'):\n",
        "    story = 'The Adventure of the Second Stain'\n",
        "    corpus = get_corpus(story)\n",
        "    corpus_l = corpus.lower()\n",
        "    spans = get_adventures(corpus_l, n=CORPUS_URL[story][1])\n",
        "    end_spans = get_end_of_roi(corpus_l, r'(\\s*\\n){2,}[*]{5,}(\\s*\\n){2,}', span=spans[0])\n",
        "    if end_spans:\n",
        "        spans = [(spans[0][0], end_spans[0][0])]\n",
        "    texts[story] = get_text_from_span(corpus, spans)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cife-Ku0q_5E",
        "outputId": "b66a30ca-7d4a-43dd-d1ab-8e2e4af6448d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Print stories info and first/last chunk of stories\n",
        "total_token_count = 0\n",
        "for idx, (story, text) in enumerate(texts.items(), start=1):\n",
        "    textc = clean_text_ws(text)\n",
        "    token_count = len(get_tokens(textc))\n",
        "    total_token_count += token_count\n",
        "    print(f'{idx}. {story}')\n",
        "    print('Token count:', token_count)\n",
        "    print(f'\\t{textc[:80]} ...')\n",
        "    print(f'\\t... {textc[-80:]}')\n",
        "    print()\n",
        "print('Total token count:', total_token_count)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. The Valley of Fear\n",
            "Token count: 57686\n",
            "\t\"I am inclined to think--\" said I. \"I should do so,\" Sherlock Holmes remarked im ...\n",
            "\t... e for some minutes, while those fateful eyes still strained to pierce the veil. \n",
            "\n",
            "2. A Study in Scarlet\n",
            "Token count: 43296\n",
            "\tIN the year 1878 I took my degree of Doctor of Medicine of the University of Lon ...\n",
            "\t... ulus me sibilat, at mihi plaudo Ipse domi simul ac nummos contemplor in arca.'\" \n",
            "\n",
            "3. The Sign of the Four\n",
            "Token count: 43050\n",
            "\tSherlock Holmes took his bottle from the corner of the mantel-piece and his hypo ...\n",
            "\t... ll remains the cocaine-bottle.\" And he stretched his long white hand up for it. \n",
            "\n",
            "4. The Hound of the Baskervilles\n",
            "Token count: 59170\n",
            "\tMr. Sherlock Holmes, who was usually very late in the mornings, save upon those  ...\n",
            "\t...  in half an hour, and we can stop at Marcini's for a little dinner on the way?\" \n",
            "\n",
            "5. The Boscombe Valley Mystery\n",
            "Token count: 9622\n",
            "\tWe were seated at breakfast one morning, my wife and I, when the maid brought in ...\n",
            "\t... e happily together in ignorance of the black cloud which rests upon their past. \n",
            "\n",
            "6. The Five Orange Pips\n",
            "Token count: 7330\n",
            "\tWhen I glance over my notes and records of the Sherlock Holmes cases between the ...\n",
            "\t... on it, and that is all which we shall ever know of the fate of the \"Lone Star.\" \n",
            "\n",
            "7. The Adventure of the Speckled Band\n",
            "Token count: 9817\n",
            "\tOn glancing over my notes of the seventy odd cases in which I have during the la ...\n",
            "\t... , and I cannot say that it is likely to weigh very heavily upon my conscience.\" \n",
            "\n",
            "8. The Adventure of the Cardboard Box\n",
            "Token count: 8758\n",
            "\t In choosing a few typical cases which illustrate the remarkable mental qualitie ...\n",
            "\t... ding perennial problem to which human reason is as far from an answer as ever.” \n",
            "\n",
            "9. The Musgave Ritual\n",
            "Token count: 7594\n",
            "\t An anomaly which often struck me in the character of my friend Sherlock Holmes  ...\n",
            "\t...  and carried herself and the memory of her crime to some land beyond the seas.” \n",
            "\n",
            "10. The Reigate Squires\n",
            "Token count: 7262\n",
            "\t It was some time before the health of my friend Mr. Sherlock Holmes recovered f ...\n",
            "\t... cess, and I shall certainly return much invigorated to Baker Street to-morrow.” \n",
            "\n",
            "11. The Adventure of the Dancing Men\n",
            "Token count: 9654\n",
            "\tHOLMES had been seated for some hours in silence with his long, thin back curved ...\n",
            "\t...  life to the care of the poor and to the administration of her husband's estate.\n",
            "\n",
            "12. The Adventure of the Second Stain\n",
            "Token count: 9674\n",
            "\tI HAD intended \"The Adventure of the Abbey Grange\" to be the last of those explo ...\n",
            "\t... our diplomatic secrets,\" said he, and picking up his hat he turned to the door. \n",
            "\n",
            "Total token count: 272913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD1Db1mTr8ZM",
        "outputId": "b83452ee-98f5-4ef1-99a1-61ea90ac64cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "merged_text = clean_text_ws('\\n'.join(texts.values()))\n",
        "print(f'\\t{merged_text[:80]} ...')\n",
        "print(f'\\t... {merged_text[-80:]}')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\"I am inclined to think--\" said I. \"I should do so,\" Sherlock Holmes remarked im ...\n",
            "\t... our diplomatic secrets,\" said he, and picking up his hat he turned to the door. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JzkcuD9tsYW"
      },
      "source": [
        ""
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgdetw4Gwkyq",
        "outputId": "8cf09759-8e94-41bb-a474-8f49c7eb0802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(list(gensim.utils.tokenize(merged_text)))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276539"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kBPu0fK-6yr"
      },
      "source": [
        "# EOF"
      ]
    }
  ]
}