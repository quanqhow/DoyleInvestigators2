{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doyle_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+YHRBhCnX040V6T51yvrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edponce/DoyleInvestigators2/blob/main/Doyle_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcHCmg6u6Sc-"
      },
      "source": [
        "# Install AuthorDetect from GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5om6LwCf6CxB"
      },
      "source": [
        "!pip install git+https://github.com/edponce/DoyleInvestigators2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt-Ni1cA6WBg"
      },
      "source": [
        "# Set up NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wp8xsoD6XtZ",
        "outputId": "a3efd596-bd45-4845-819d-e13be74938c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# Sentencizer\n",
        "nltk.download('punkt')\n",
        "# Tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Universal POS tags\n",
        "nltk.download('universal_tagset')\n",
        "# Stemmers\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCEZJFyaeybc"
      },
      "source": [
        "# Reproducibility Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDfRRsf-eWqz"
      },
      "source": [
        "#PYTHONHASHSEED = 0\n",
        "global_seed = None"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfrw4yW7PGu"
      },
      "source": [
        "# Imports and mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOTDCfXU6Ot3"
      },
      "source": [
        "from authordetect import (\n",
        "    Author, Tokenizer, EmbeddingModel, Classifier,\n",
        "    save_pickle, load_pickle, save_json, load_json, np_avg, np_sum,\n",
        ")\n",
        "import numpy\n",
        "import random\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "from typing import Any, Dict, Union, Iterable"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86B2Nhgz7Loy",
        "outputId": "fca469b7-2535-4543-e7ed-2ed1f902f25a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JjuGw549U__"
      },
      "source": [
        "# Author Vector Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0xp0ZWA9uVg"
      },
      "source": [
        "## Writer2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz48LQIm8u8_"
      },
      "source": [
        "### Method 1: Load existing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emMgwT3J8mBa"
      },
      "source": [
        "embedding_infile = '/content/gdrive/My Drive/Doyle Investigators 2/w2v/doyle_50dim_350part.bin'\n",
        "embedding = EmbeddingModel(embedding_infile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp6DZkwJ9DLG"
      },
      "source": [
        "### Method 2: Train new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzuLtnjY8olp"
      },
      "source": [
        "embedding = None"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JLy9xOtBgWY"
      },
      "source": [
        "### Embedding parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SEHnIYk8ZO6"
      },
      "source": [
        "writer2vec_params = {\n",
        "    # Tokenizer\n",
        "    'tokenizer': Tokenizer(lemmatizer='wordnet'),\n",
        "\n",
        "    # Document partitioning\n",
        "    'part_size': 350,  # int, None=for standalong documents\n",
        "    # 'remain_factor': 350/350,  # float [0,1], default=1\n",
        "\n",
        "    # word2vec - Parameters passed directly to gensim.models.Word2Vec\n",
        "    # https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
        "    'size': 50,\n",
        "    'window': 12,\n",
        "    'min_count': 1,\n",
        "    # 'sg': 0,\n",
        "    # 'hs': 0,\n",
        "    'negative': 200,\n",
        "    'alpha': 0.05,\n",
        "    # 'min_alpha': 0.0007,\n",
        "    'seed': global_seed,\n",
        "    'sample': 6e-5,\n",
        "    'iter': 5,\n",
        "\n",
        "    # word2vec embedding\n",
        "    'embedding': embedding,\n",
        "    \n",
        "    # doc2vec\n",
        "    'stopwords': Tokenizer.STOPWORDS,  # iterable[str], None\n",
        "    'func': np_avg,  # callable\n",
        "    'use_norm': True,  # bool\n",
        "    'missing_value': 0,  # int\n",
        "}"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkwOjI7qJ6zN"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Ig75oa9wgu",
        "outputId": "be6b7f13-563e-4eb8-b41b-524f1766b9f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corpus_file = '/content/gdrive/My Drive/Doyle Investigators 2/raw/Doyle_10.txt'\n",
        "\n",
        "main_author = Author(corpus_file)\n",
        "main_author.writer2vec(**writer2vec_params)\n",
        "\n",
        "print('Corpus characters:', len(main_author.text))\n",
        "print('Corpus sentences:', len(main_author.sentences))\n",
        "print('Corpus words:', len(main_author.words))\n",
        "print('Corpus vocabulary:', len(main_author.parsed_text.vocabulary))\n",
        "print('Embedding vocabulary:', len(main_author.embedding.vocabulary))\n",
        "print('Embedding matrix:', main_author.embedding.vectors.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author input will be loaded from a file\n",
            "Corpus characters: 149008\n",
            "Corpus sentences: 1856\n",
            "Corpus words: 28077\n",
            "Corpus vocabulary: 3110\n",
            "Embedding vocabulary: 3110\n",
            "Embedding matrix: (3110, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usyz1O4dJ-wt"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux2t44EDBeaT"
      },
      "source": [
        "embedding_outfile = '/content/gdrive/My Drive/Doyle Investigators 2/w2v/doyle_50dim_350part.bin'\n",
        "if embedding_outfile:\n",
        "    main_author.embedding.save(embedding_outfile)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb8AFUbHDOT4"
      },
      "source": [
        "## Example: Word vector embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wnYl02nDpc_"
      },
      "source": [
        "### Method 1: AuthorDetect API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iidGRzaeDUNU",
        "outputId": "f9665162-576b-446b-b9f5-fde0f334790a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "main_author.embedding.vectors\n",
        "vec = main_author.embedding['holmes']\n",
        "print(vec)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.46549737  0.60374576  0.8470459  -0.19474304 -1.3384175  -0.81365055\n",
            "  0.28888994  0.62765944  0.2118628  -1.1505791   0.35912797  0.29321107\n",
            "  0.04541311 -0.01090856  1.0617627  -0.713252   -0.482456    0.86590815\n",
            " -0.6698707   0.51217306 -1.8313785   0.15041183  1.4251413   1.2170594\n",
            "  1.1220206   0.92609    -0.699016    0.7248917  -1.3157141  -1.4904219\n",
            "  0.45005497  1.2250013  -0.55145645  1.5065546   1.0859183   0.7529688\n",
            "  0.49549443 -0.35432148 -1.1073892  -0.14995696 -0.13276358 -0.44677708\n",
            "  0.34673393 -1.3918135  -1.155032   -0.332819   -0.62239987 -0.14582188\n",
            "  0.51104206  0.87963104]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY3LFq8HDtTI"
      },
      "source": [
        "### Method 2: Gensim API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf6FdWNYD3Lh",
        "outputId": "9261c69d-c2da-4717-821d-b35c5e80335f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w2v_model = main_author.embedding.model\n",
        "vec = w2v_model.wv['holmes']\n",
        "print(vec)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.46549737  0.60374576  0.8470459  -0.19474304 -1.3384175  -0.81365055\n",
            "  0.28888994  0.62765944  0.2118628  -1.1505791   0.35912797  0.29321107\n",
            "  0.04541311 -0.01090856  1.0617627  -0.713252   -0.482456    0.86590815\n",
            " -0.6698707   0.51217306 -1.8313785   0.15041183  1.4251413   1.2170594\n",
            "  1.1220206   0.92609    -0.699016    0.7248917  -1.3157141  -1.4904219\n",
            "  0.45005497  1.2250013  -0.55145645  1.5065546   1.0859183   0.7529688\n",
            "  0.49549443 -0.35432148 -1.1073892  -0.14995696 -0.13276358 -0.44677708\n",
            "  0.34673393 -1.3918135  -1.155032   -0.332819   -0.62239987 -0.14582188\n",
            "  0.51104206  0.87963104]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jxzlnWS66OF"
      },
      "source": [
        "# Writer2Vec - Document Vector Embedding Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jnvBtZZ613Q"
      },
      "source": [
        "def writer2vec(data: Union[str, Iterable[str]], labels: Union[Any, Iterable[Any]], **kwargs):\n",
        "    if isinstance(data, str):\n",
        "        data = [data]\n",
        "        labels = [labels]\n",
        "\n",
        "    authors = []\n",
        "    for i, (corpus, label) in enumerate(zip(data, labels), start=1):\n",
        "        author = Author(corpus, label)\n",
        "        author.writer2vec(**kwargs)\n",
        "        authors.append(author)\n",
        "    return authors\n",
        "\n",
        "\n",
        "def split_combine_data(data, labels, pos_frac=0.5, neg_frac=None, *, seed=None):\n",
        "    \"\"\"Split a nested dataset of the form [[pos], [neg], [neg], ...],\n",
        "    using the fractions provided.\n",
        "\n",
        "    The positive data is first and is used completely with the pos_frac\n",
        "    stating the fraction it represents. Partial negative datasets are\n",
        "    generated via random sampling. Data is returned in the same format.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    total_size = len(data[0]) / pos_frac\n",
        "\n",
        "    # Calculate negative fraction, uniformly for each negative dataset\n",
        "    _neg_frac = (1 - pos_frac) / (len(data) - 1)\n",
        "    if neg_frac is not None:\n",
        "        _neg_frac = min(_neg_frac, neg_frac)\n",
        "    neg_frac = _neg_frac\n",
        "\n",
        "    # Randomly select negative data\n",
        "    combined_data = [data[0]]\n",
        "    combined_labels = [labels[0]]\n",
        "    neg_size = int(total_size * neg_frac)\n",
        "    for neg_data, neg_labels in zip(data[1:], labels[1:]):\n",
        "        _neg_size = min(neg_size, len(neg_data))\n",
        "        idxs = random.sample(range(len(neg_data)), _neg_size)\n",
        "        _neg_data = [neg_data[i] for i in idxs]\n",
        "        _neg_labels = [neg_labels[i] for i in idxs]\n",
        "        combined_data.append(_neg_data)\n",
        "        combined_labels.append(_neg_labels)\n",
        "\n",
        "    return combined_data, combined_labels\n",
        "\n",
        "\n",
        "def flatten(data):\n",
        "    return numpy.array(list(itertools.chain.from_iterable(data)))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IqYDytfKOeQ"
      },
      "source": [
        "## Train model using 90% from 10/90 split of original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW_h6i2pBjGD"
      },
      "source": [
        "# Positive author first\n",
        "train_data = [\n",
        "    '/content/gdrive/My Drive/Doyle Investigators 2/raw/Doyle_90.txt',\n",
        "    '/content/gdrive/My Drive/Doyle Investigators 2/raw/Rinehart_90.txt',\n",
        "    '/content/gdrive/My Drive/Doyle Investigators 2/raw/Christie_90.txt',\n",
        "]\n",
        "train_labels = [1, 0, 0]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aTdCBtTF3GX",
        "outputId": "58394792-2b40-4a5f-e043-48b68c3292b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "writer2vec_params['embedding'] = main_author.embedding\n",
        "\n",
        "# Document vectors and labels\n",
        "train_authors = writer2vec(train_data, train_labels, **writer2vec_params)\n",
        "\n",
        "# Extract vectors and labels\n",
        "vectors = []\n",
        "labels = []\n",
        "for author in train_authors:\n",
        "    vectors.append(author.dv)\n",
        "    labels.append([author.label] * len(author.dv))\n",
        "\n",
        "# Fraction select (50% of 90% for positive, 25% of 90% for each negative)\n",
        "vectors, labels = split_combine_data(vectors, labels, seed=global_seed)\n",
        "\n",
        "# Flatten data\n",
        "train_vectors = flatten(vectors)\n",
        "train_labels = flatten(labels)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author input will be loaded from a file\n",
            "Author input will be loaded from a file\n",
            "Author input will be loaded from a file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cfP5a1R7iX3"
      },
      "source": [
        "# MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FferpWxh7l0a"
      },
      "source": [
        "def train_classifier(vectors: Iterable[Iterable[float]], labels: Iterable[int], train_test_params: Dict[str, Any] = None, **kwargs) -> 'MLPClassifier':\n",
        "    if train_test_params is None:\n",
        "        train_test_params = {}\n",
        "\n",
        "    vectors = numpy.array(vectors)\n",
        "    labels = numpy.array(labels)\n",
        "    train_vectors, test_vectors, train_labels, test_labels = train_test_split(\n",
        "        vectors, labels,\n",
        "        stratify=labels,\n",
        "        **train_test_params,\n",
        "    )\n",
        "\n",
        "    mlp = MLPClassifier(**kwargs)\n",
        "    mlp.fit(train_vectors, train_labels)\n",
        "\n",
        "    return mlp"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBfsv4ZMKTfe"
      },
      "source": [
        "## Classifier parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R0K-2ByffpW"
      },
      "source": [
        "# Train/test data split - Parameters passed directly to sklearn.model_selection.train_test_split\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
        "train_test_params = {\n",
        "    'test_size': 0.1,  # train_size=1-test_size\n",
        "    'random_state': global_seed,\n",
        "}\n",
        "\n",
        "# Classifier - Paramaters passed directly to sklearn.neural_network.MLPClassifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
        "mlp_params = {\n",
        "    'hidden_layer_sizes': (50,50,50),\n",
        "    # 'activation': 'relu',\n",
        "    'solver': 'adam',\n",
        "    'alpha': 1e-4,\n",
        "    'random_state': global_seed,\n",
        "    # 'learning_rate': 'constant',  # only used when solver='sgd'\n",
        "    'max_iter': 2000,\n",
        "    # 'shuffle': True,  # only used when solver='sgd' or 'adam'\n",
        "    # 'warm_start': False,\n",
        "    # 'momentum': 0.9,  # only used when solver='sgd'\n",
        "    # 'max_fun': 15000,  # only used when solver='lbfgs'\n",
        "}"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBGQ3M_wKfyi"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24A__f6K6DN"
      },
      "source": [
        "### Method 1: Load existing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4UQJeuiKs-z"
      },
      "source": [
        "mlp_infile = '/content/gdrive/My Drive/Doyle Investigators 2/mlp/doyle_50dim_350part.pkl'\n",
        "mlp = load_pickle(mlp_infile)\n",
        "print(mlp.coefs_[0].shape)\n",
        "print(mlp.coefs_[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ZxsYOpK_YI"
      },
      "source": [
        "### Method 2: Train new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkLwJ5-DF_5j"
      },
      "source": [
        "mlp = train_classifier(train_vectors, train_labels, train_test_params, **mlp_params)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4V6Jhy6Kka7"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7p1TDEQGBp0"
      },
      "source": [
        "mlp_outfile = '/content/gdrive/My Drive/Doyle Investigators 2/mlp/doyle_50dim_350part.pkl'\n",
        "if mlp_outfile:\n",
        "    save_pickle(mlp, mlp_outfile)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IupXHLX7qCPk"
      },
      "source": [
        "# MLP Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuplWTaBI4xl"
      },
      "source": [
        "def predict(mlp: 'MLPClassifier', vectors: Iterable[Iterable[float]], true_labels: Iterable[Any] = None):\n",
        "    predict_labels = mlp.predict(vectors)\n",
        "    probabilities = mlp.predict_proba(vectors)\n",
        "\n",
        "    if true_labels is not None:\n",
        "        score = mlp.score(vectors, true_labels)\n",
        "        f1 = f1_score(true_labels, predict_labels, zero_division=1)\n",
        "        precision, recall, fbeta, support = precision_recall_fscore_support(\n",
        "            true_labels, predict_labels, zero_division=1\n",
        "        )\n",
        "        metrics = {\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'fbeta': fbeta,\n",
        "        }\n",
        "    else:\n",
        "        metrics = None\n",
        "\n",
        "    return predict_labels, mlp.classes_, probabilities, metrics\n",
        "\n",
        "\n",
        "def get_perturbed_data(docfile: str, label_map: dict = None):\n",
        "    docs = []\n",
        "    labels = []\n",
        "    for doc in load_json(docfile):       \n",
        "        label = label_map[doc['label'].lower()] if label_map else doc['label']\n",
        "        labels.append(label)\n",
        "        docs.append(doc['text'])\n",
        "    return docs, labels"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wzv5uUQI1EK"
      },
      "source": [
        "## Method 1: Predict using 10% from 10/90 split of original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHoDhX7IL7E6"
      },
      "source": [
        "test_data = [\n",
        "    '/content/gdrive/My Drive/Doyle Investigators 2/raw/Doyle_10.txt',\n",
        "    '/content/gdrive/My Drive/Doyle Investigators 2/raw/Christie_10.txt',\n",
        "    '/content/gdrive/My Drive/Doyle Investigators 2/raw/Rinehart_10.txt',\n",
        "]\n",
        "\n",
        "test_labels = [1, 0, 0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_A0Uq2fg3rZ"
      },
      "source": [
        "## Method 2: Predict using perturbed test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvPEd869lzt_"
      },
      "source": [
        "LABEL_MAP = {\n",
        "    'doyle': 1,\n",
        "    'christie': 0,\n",
        "    'rinehart': 0,\n",
        "}\n",
        "\n",
        "test_file = '/content/gdrive/My Drive/Doyle Investigators 2/perturbed/perturbed_synonym_doyle_350.json'\n",
        "test_file = '/content/gdrive/My Drive/Doyle Investigators 2/perturbed/perturbed_numericals_doyle_350.json'\n",
        "\n",
        "test_data, test_labels = get_perturbed_data(test_file, LABEL_MAP)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlISBV3FsV-e"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg1h4JEhsdIi",
        "outputId": "35489877-afc5-49d7-cc5f-60111bf2ab6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Document vectors and labels\n",
        "test_authors = writer2vec(test_data, test_labels, **writer2vec_params)\n",
        "\n",
        "# Extract vectors and labels\n",
        "vectors = []\n",
        "labels = []\n",
        "for author in test_authors:\n",
        "    vectors.append(author.dv)\n",
        "    labels.append([author.label] * len(author.dv))\n",
        "\n",
        "# Flatten data\n",
        "test_vectors = flatten(vectors)\n",
        "test_labels = flatten(labels)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n",
            "Author input was provided as raw text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6aNx-2NMc83",
        "outputId": "f9f9c37a-c013-4b31-c67a-1b8dcb0c1226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predict_labels, classes, probabilities, metrics = predict(mlp, test_vectors, test_labels)\n",
        "print('True:', test_labels)\n",
        "print('Predict:', predict_labels)\n",
        "# print('Classes:', classes)\n",
        "# print('Probabilities:', probabilities)\n",
        "print(metrics)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True: [0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "Predict: [0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1]\n",
            "{'f1': 0.6578947368421053, 'precision': array([0.97849462, 0.49668874]), 'recall': array([0.54491018, 0.97402597]), 'fbeta': array([0.7       , 0.65789474])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}