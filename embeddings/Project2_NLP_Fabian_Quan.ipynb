{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2_NLP_Fabian_Quan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLriY8QzaySHJZ82lZvgzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edponce/DoyleInvestigators2/blob/main/Project2_NLP_Fabian_Quan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t3zrbX5so46"
      },
      "source": [
        "#$\\color{brown}{\\rm 1.~Preprocess~Data}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta-voKpu4Q2Y"
      },
      "source": [
        "###1.1. Load Data and remove null spaces in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgalYWRVrzyP",
        "outputId": "7e586d31-68e9-43ca-a52d-456da80ba230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import re  # For preprocessing\n",
        "import pandas as pd  # For data handling\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import spacy  # For preprocessing\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "#1. Toy example with the script of 27 seasons of the scripts from the tv show \"The Simpsons\"\n",
        "df = pd.read_csv('toy_example_corpora_simpsons.csv')\n",
        "print(df.shape)\n",
        "\n",
        "#2. This dataset has many null rows, this line gets rid of it\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 03:02:57: NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(158314, 2)\n",
            "(131853, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK_9t9zI4Zgn"
      },
      "source": [
        "###1.2. Lemmatization and basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jjTPErEt7AN",
        "outputId": "477b4fd0-b8cc-4609-946a-749ae7cca89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#1. Disabling Named Entity Recognition just for speed\n",
        "nlp = spacy.load('en', disable=['ner', 'parser']) \n",
        "\n",
        "#2. Utility functions\n",
        "def cleaning(doc):\n",
        "    # doc needs to be a spacy Doc object\n",
        "\n",
        "    #Lemmatizes and removes stopwords\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    if len(txt) > 2:\n",
        "        return ' '.join(txt)\n",
        "\n",
        "#Remove non-alphabetic characters for each line of dialogue\n",
        "#This line creates a generator\n",
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])\n",
        "\n",
        "#3. Run the cleaning over all rows\n",
        "#Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process\n",
        "sentences = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
        "\n",
        "print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "131853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxMOZ6D84fmB"
      },
      "source": [
        "###1.3. Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.\n",
        "\n",
        "Example: to catch things like \"mr_holmes\" in sentences with \"Mr. Holmes\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVu4KVzf3PEh",
        "outputId": "e0741d68-fe36-48fb-e5b3-0f67a0f492fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "#clean sentences with None type and duplicates\n",
        "df_clean = pd.DataFrame({'clean': sentences})\n",
        "print(df_clean.shape)\n",
        "df_clean = df_clean.dropna().drop_duplicates()\n",
        "print(df_clean.shape)\n",
        "\n",
        "#Prepare data: as Phrases() takes a list of list of words as input\n",
        "sent = [row.split() for row in df_clean['clean']]\n",
        "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "\n",
        "#The goal of Phraser() is to cut down memory consumption of Phrases()\n",
        "#Transform the corpus based on the bigrams detected\n",
        "bigram = Phraser(phrases)\n",
        "clean_sentences = bigram[sent]\n",
        "\n",
        "print()\n",
        "print(len(clean_sentences))\n",
        "print(clean_sentences[7])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 03:04:48: collecting all words and their counts\n",
            "INFO - 03:04:48: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(131853, 1)\n",
            "(85964, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO - 03:04:48: PROGRESS: at sentence #10000, processed 63561 words and 52816 word types\n",
            "INFO - 03:04:48: PROGRESS: at sentence #20000, processed 130943 words and 99866 word types\n",
            "INFO - 03:04:48: PROGRESS: at sentence #30000, processed 192972 words and 138532 word types\n",
            "INFO - 03:04:48: PROGRESS: at sentence #40000, processed 249842 words and 172659 word types\n",
            "INFO - 03:04:49: PROGRESS: at sentence #50000, processed 311265 words and 208566 word types\n",
            "INFO - 03:04:49: PROGRESS: at sentence #60000, processed 373588 words and 243702 word types\n",
            "INFO - 03:04:49: PROGRESS: at sentence #70000, processed 436441 words and 278740 word types\n",
            "INFO - 03:04:49: PROGRESS: at sentence #80000, processed 497829 words and 311886 word types\n",
            "INFO - 03:04:49: collected 330804 word types from a corpus of 537160 words (unigram + bigrams) and 85964 sentences\n",
            "INFO - 03:04:49: using 330804 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 03:04:49: source_vocab length 330804\n",
            "INFO - 03:04:52: Phraser built with 126 phrasegrams\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "85964\n",
            "['think', 'take', 'train', 'capital_city']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P_Sf4O67xVw"
      },
      "source": [
        "#$\\color{brown}{\\rm 2.~Train~Model}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqG6mdzg8BXe"
      },
      "source": [
        "###2.1. Word2Vec setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD_zBiSy8LQZ"
      },
      "source": [
        "#Gensim Word2Vec Implementation\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#Parameter of word2vec\n",
        "'''\n",
        "min_count =  int   --> Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "window =     int   --> The maximum distance between the current and predicted word within a sentence. \n",
        "                       E.g. window words on the left and window words on the right of our target - (2, 10)\n",
        "size =       int   --> Dimensionality of the feature vectors. - (50, 300)\n",
        "sample =     float --> The threshold for configuring which higher-frequency words are randomly downsampled. \n",
        "                       Highly influencial. - (0, 1e-5)\n",
        "alpha =      float --> The initial learning rate - (0.01, 0.05)\n",
        "min_alpha =  float --> Learning rate will linearly drop to min_alpha as training progresses. \n",
        "                       To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "negative =   int   --> If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. \n",
        "                       If set to 0, no negative sampling is used. - (5, 20)\n",
        "workers =    int   --> Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "sg =         int   -->  CBOW (0) -default- or skip gram (1)\n",
        "'''\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bze2AYqk8Tj9"
      },
      "source": [
        "###2.2. Building the Vocabulary Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oPA9-Yh8a01",
        "outputId": "76ac5f22-06db-4c52-e48f-17bbfe252f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#Word2Vec requires us to build the vocabulary table \n",
        "#(simply digesting all the words and filtering out the unique words, and doing some basic counts on them):\n",
        "w2v_model.build_vocab(clean_sentences, progress_per=10000)\n",
        "\n",
        "print()\n",
        "print(w2v_model.corpus_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 03:05:02: collecting all words and their counts\n",
            "INFO - 03:05:02: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 03:05:02: PROGRESS: at sentence #10000, processed 61718 words, keeping 9558 word types\n",
            "INFO - 03:05:02: PROGRESS: at sentence #20000, processed 127351 words, keeping 14506 word types\n",
            "INFO - 03:05:03: PROGRESS: at sentence #30000, processed 187829 words, keeping 17619 word types\n",
            "INFO - 03:05:03: PROGRESS: at sentence #40000, processed 243332 words, keeping 20385 word types\n",
            "INFO - 03:05:03: PROGRESS: at sentence #50000, processed 303182 words, keeping 22878 word types\n",
            "INFO - 03:05:03: PROGRESS: at sentence #60000, processed 363940 words, keeping 25200 word types\n",
            "INFO - 03:05:04: PROGRESS: at sentence #70000, processed 425408 words, keeping 27401 word types\n",
            "INFO - 03:05:04: PROGRESS: at sentence #80000, processed 485464 words, keeping 29275 word types\n",
            "INFO - 03:05:04: collected 30178 word types from a corpus of 523700 raw words and 85964 sentences\n",
            "INFO - 03:05:04: Loading a fresh vocabulary\n",
            "INFO - 03:05:04: effective_min_count=20 retains 3319 unique words (10% of original 30178, drops 26859)\n",
            "INFO - 03:05:04: effective_min_count=20 leaves 437324 word corpus (83% of original 523700, drops 86376)\n",
            "INFO - 03:05:04: deleting the raw counts dictionary of 30178 items\n",
            "INFO - 03:05:04: sample=6e-05 downsamples 1200 most-common words\n",
            "INFO - 03:05:04: downsampling leaves estimated 199161 word corpus (45.5% of prior 437324)\n",
            "INFO - 03:05:04: estimated required memory for 3319 words and 300 dimensions: 9625100 bytes\n",
            "INFO - 03:05:04: resetting layer weights\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "85964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDtjQWhj9Obk"
      },
      "source": [
        "###2.3. Training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyh7FgKJ9R99"
      },
      "source": [
        "#Parameters of the training:\n",
        "#       >> total_examples = int -> Count of sentences;\n",
        "#       >> epochs = int -> Number of iterations (epochs) over the corpus - [10, 20, 30]\n",
        "\n",
        "w2v_model.train(clean_sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqPlaVA8Accp",
        "outputId": "fdc3b66b-7469-48a2-8c05-6c2061e745e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#As we do not plan to train the model any further, we are calling init_sims(), \n",
        "#which will make the model much more memory-efficient:\n",
        "#------------NOT SURE IF THIS LINE IS NECESSARY BEFORE SAVING THE MODEL---------\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 03:10:38: precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbvKexrhCC7m"
      },
      "source": [
        "###2.4. Save the model (for Jerry and Chris?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSjLcr9eCP_Q",
        "outputId": "06afd45d-b9ea-43df-e5a0-21d62a2ecf24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "w2v_model.wv.save_word2vec_format('model_author_name.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 03:10:41: storing 3319x300 projection weights into model_author_name.bin\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty6jaH-oAwJF"
      },
      "source": [
        "#$\\color{brown}{\\rm 3.~Get~Embeddings~for~Classification}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUDluKnrA6Zk",
        "outputId": "61b56c03-0f1a-4e0f-a691-4e42e52f5779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#EDUARDO AND MAOFENG\n",
        "#Here we call the method that will get all random words from\n",
        "\n",
        "#This is just an example of getting embeddings from one word\n",
        "import numpy as np\n",
        "arrays = np.empty((0, 300), dtype='f')\n",
        "arrays = np.append(arrays, w2v_model.wv.__getitem__(['homer']), axis=0)\n",
        "arrays = np.append(arrays, w2v_model.wv.__getitem__(['marge']), axis=0)\n",
        "\n",
        "print(arrays.shape)\n",
        "\n",
        "#TODO: CONSTRUCT THE WHOLE DATASET TO FEED THE CLASSIFIER(S)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQVLzO_XRdtS"
      },
      "source": [
        "#$\\color{brown}{\\rm 4.~Train~the~classifiers}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlAW9RkBRk5l"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}